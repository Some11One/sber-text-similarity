{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "from nltk.collocations import *\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import functools\n",
    "import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from itertools import product\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk, string\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pymorphy2\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "# exclude = set(punctuation + u'0123456789[]—«»–')\n",
    "exclude = set(punctuation + u'[]—«»–')\n",
    "import pylanguagetool\n",
    "import requests\n",
    "import json\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re1 = re.compile(\"\"\"\n",
    "    (?:\n",
    "        (?:\n",
    "            (?<!\\\\d(?:р|г|к))\n",
    "            (?<!и\\\\.т\\\\.(?:д|п))\n",
    "            (?<!и(?=\\\\.т\\\\.(?:д|п)\\\\.))\n",
    "            (?<!и\\\\.т(?=\\\\.(?:д|п)\\\\.))\n",
    "            (?<!руб|коп)\n",
    "        \\\\.) |\n",
    "        [!?\\\\n]\n",
    "    )+\n",
    "    \"\"\", re.X)\n",
    "\n",
    "def safe_split(regex, text):\n",
    "    res = []\n",
    "    sear = regex.search(text)\n",
    "    while sear:\n",
    "        res.append(text[:sear.end()])\n",
    "        text = text[sear.end():]\n",
    "        sear = regex.search(text)\n",
    "    res.append(text)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_patterns(text):\n",
    "    tokens = text.split(' ')\n",
    "    return [[p.tag.POS for p in morph.parse(word)][0] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(text):\n",
    "    return ' '.join(re.findall(r\"\\w+\", text)).lower()\n",
    "\n",
    "def normalize_answer_no_lower(text):\n",
    "    return ' '.join(re.findall(r\"\\w+\", text))\n",
    "\n",
    "def sentence_to_word(sentences):\n",
    "    sentences_in_words = list()\n",
    "    for sentence in sentences:\n",
    "        sentences_in_words.append(normalize_answer(sentence).split())\n",
    "    return sentences_in_words\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    sentences = safe_split(re1, text)\n",
    "    return [s.strip() for s in sentences if s.strip() != '']\n",
    "\n",
    "def get_max_match_sentance(data_row, w2v = False):\n",
    "    sentences = text_to_sentence(data_row[\"paragraph\"])\n",
    "    sentences_in_words = sentence_to_word(sentences)\n",
    "    question_in_words = sentence_to_word([data_row[\"question\"]])[0]\n",
    "    overlaps = []\n",
    "    question_words = set(question_in_words)\n",
    "    for sentance_id in range(len(sentences_in_words)):\n",
    "        sentence_words = set(sentences_in_words[sentance_id])\n",
    "        if w2v:\n",
    "            overlap = texts_intersection(sentence_words, question_words, model, 0.2)\n",
    "        else:\n",
    "            overlap = sentence_words.intersection(question_words)\n",
    "        overlaps.append(overlap)\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=2 ** 19)\n",
    "def uniq_words(text):\n",
    "    return set(re.findall(\"\\w+\", text))\n",
    "\n",
    "def calculate_idfs(data):\n",
    "    counter = Counter()\n",
    "    uniq_paragraphs = data['paragraph'].unique()\n",
    "    uniq_questions = data['question'].unique()\n",
    "    for paragraph in tqdm.tqdm(uniq_paragraphs, desc=\"calc idf for paragraph\"):\n",
    "        set_words = uniq_words(paragraph)\n",
    "        counter.update(set_words)\n",
    "    for question in tqdm.tqdm(uniq_questions, desc=\"calc idf for question\"):\n",
    "        set_words = uniq_words(question)\n",
    "        counter.update(set_words)\n",
    "    num_docs = uniq_paragraphs.shape[0] + uniq_questions.shape[0]\n",
    "    idfs = {}\n",
    "    for word in counter:\n",
    "        idfs[word] = np.log(num_docs / counter[word])\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    buf = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = WhitespaceTokenizer().tokenize(buf.lower())\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    lemmas = []\n",
    "    for t in tokens[:]:\n",
    "        if not t in stopwords.words('russian'):\n",
    "            try:\n",
    "                lemma = lemmatizer.parse(t)[0].normal_form\n",
    "            except: \n",
    "                lemma = t\n",
    "            lemmas.append(lemma)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texts_intersection(text1, text2, word2vec, threshold):\n",
    "    words1 = set(text1)\n",
    "    words2 = set(text2)\n",
    "    result = []\n",
    "    for word1 in words1:\n",
    "        for word2 in words2:\n",
    "            if word1 == word2:\n",
    "                similarity = 1.0\n",
    "            elif word1 in word2vec and word2 in word2vec:\n",
    "                similarity = word2vec.similarity(word1, word2)\n",
    "            else:\n",
    "                similarity = 0.0\n",
    "            if similarity >= threshold:\n",
    "                result.append(word1)\n",
    "                result.append(word2)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = ['ADJF+NOUN','PRTS+NOUN','NOUN+NOUN', 'NOUN+VERB', 'VERB+ADJF', 'VERB+NOUN']\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def match(ngram, patterns = patterns):\n",
    "    index = []\n",
    "    for word in ngram:\n",
    "        buf = [(p.normal_form, p.tag.POS) for p in morph.parse(word)]\n",
    "        index.append((word,buf))\n",
    "    pos_tagging = product(*[ind[1] for ind in index])\n",
    "    possible_patterns = map(lambda pos_tag: list(zip(*pos_tag)), pos_tagging)\n",
    "    possible_patterns = map(lambda pattern: [pattern[0], map(lambda grammeme: grammeme, pattern[1])], possible_patterns)\n",
    "    possible_patterns = map(lambda pattern: (pattern[0], '+'.join(pattern[1])), possible_patterns)\n",
    "    for pattern in possible_patterns:\n",
    "        if pattern[1] in patterns:\n",
    "            return pattern\n",
    "    return None        \n",
    "\n",
    "def get_pattern(text):\n",
    "    tokens = [x for x in normalize(text)]\n",
    "    pattern_coll2  = nltk.FreqDist() \n",
    "    coll2 = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    for c2 in coll2:\n",
    "        try:\n",
    "            p = match(c2)\n",
    "            if p != None:\n",
    "                collocation = ' '.join(p[0])\n",
    "                if collocation in pattern_coll2:\n",
    "                    pattern_coll2[collocation] += coll2[c2]\n",
    "                else:\n",
    "                    pattern_coll2[collocation] = coll2[c2]\n",
    "        except:\n",
    "            pass\n",
    "    return pattern_coll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftrain, dftest = pd.read_csv(\"data/train_task1_latest.csv\"), pd.read_csv(\"data/sdsj_A_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # add pattern generated questions\n",
    "# dftrain_generated = pd.read_csv(\"../data/taskA_generated_questions_p.csv\", sep=';')\n",
    "# question_id = np.max([dftrain.question_id.max(), dftest.question_id.max()]) + 1\n",
    "# for row in dftrain_generated.iterrows():\n",
    "#     dftrain_generated.loc[row[0], 'question_id'] = question_id\n",
    "#     question_id += 1\n",
    "# dftrain_generated['target'] = 0.0\n",
    "# dftrain_generated['generated_p'] = 1.0\n",
    "# dftrain_generated.columns = ['paragraph_id', 'paragraph', 'question', 'question_id', 'target', 'generated_p']\n",
    "# dftrain['generated_p'] = 0.0\n",
    "# dftrain = dftrain.append(dftrain_generated, ignore_index=True)\n",
    "# dftrain.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # add pattern_2 generated questions\n",
    "# dftrain_generated = pd.read_csv(\"../data/taskA_generated_questions_p_2.csv\", sep=';')\n",
    "# dftrain_generated = dftrain_generated.sample(10000)\n",
    "# question_id = np.max([dftrain.question_id.max(), dftest.question_id.max()]) + 1\n",
    "# for row in dftrain_generated.iterrows():\n",
    "#     dftrain_generated.loc[row[0], 'question_id'] = question_id\n",
    "#     question_id += 1\n",
    "# dftrain_generated['target'] = 0.0\n",
    "# dftrain_generated['generated_p_2'] = 1.0\n",
    "# dftrain_generated.columns = ['paragraph_id', 'paragraph', 'question', 'question_id', 'target', 'generated_p_2']\n",
    "# dftrain['generated_p_2'] = 0.0\n",
    "# dftrain = dftrain.append(dftrain_generated, ignore_index=True)\n",
    "# dftrain.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add pattern_3 generated questions\n",
    "dftrain_generated = pd.read_csv(\"../data/taskA_generated_questions_p_3.csv\", sep=';')\n",
    "dftrain_generated = dftrain_generated.sample(10000)\n",
    "question_id = np.max([dftrain.question_id.max(), dftest.question_id.max()]) + 1\n",
    "for row in dftrain_generated.iterrows():\n",
    "    dftrain_generated.loc[row[0], 'question_id'] = question_id\n",
    "    question_id += 1\n",
    "dftrain_generated['target'] = 0.0\n",
    "dftrain_generated['generated_p_3'] = 1.0\n",
    "dftrain_generated.columns = ['paragraph_id', 'paragraph', 'question', 'question_id', 'target', 'generated_p_3']\n",
    "dftrain['generated_p_3'] = 0.0\n",
    "dftrain = dftrain.append(dftrain_generated, ignore_index=True)\n",
    "dftrain.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add pattern_4 generated questions\n",
    "dftrain_generated = pd.read_csv(\"../data/taskA_generated_questions_p_4.csv\", sep=';')\n",
    "dftrain_generated = dftrain_generated.sample(10000)\n",
    "question_id = np.max([dftrain.question_id.max(), dftest.question_id.max()]) + 1\n",
    "for row in dftrain_generated.iterrows():\n",
    "    dftrain_generated.loc[row[0], 'question_id'] = question_id\n",
    "    question_id += 1\n",
    "dftrain_generated['target'] = 0.0\n",
    "dftrain_generated['generated_p_4'] = 1.0\n",
    "dftrain_generated.columns = ['paragraph_id', 'paragraph', 'question', 'question_id', 'target', 'generated_p_4']\n",
    "dftrain['generated_p_4'] = 0.0\n",
    "dftrain = dftrain.append(dftrain_generated, ignore_index=True)\n",
    "dftrain.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calc idf for paragraph: 100%|██████████| 10705/10705 [00:01<00:00, 9702.57it/s]\n",
      "calc idf for question: 100%|██████████| 120483/120483 [00:04<00:00, 29182.71it/s]\n"
     ]
    }
   ],
   "source": [
    "idfs = calculate_idfs(dftrain.append(dftest, ignore_index = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build features for train: 100%|██████████| 159171/159171 [15:19<00:00, 173.08it/s]\n",
      "build features for test: 100%|██████████| 74286/74286 [05:06<00:00, 242.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"build features for \" + name):\n",
    "        question = uniq_words(row.question)\n",
    "        paragraph = uniq_words(row.paragraph)\n",
    "        df.loc[index, 'len_paragraph'] = len(paragraph)\n",
    "        df.loc[index, 'len_question'] = len(question)\n",
    "        df.loc[index, 'len_intersection'] = len(paragraph & question)\n",
    "        df.loc[index, 'idf_question'] = np.sum([idfs.get(word, 0.0) for word in question])\n",
    "        df.loc[index, 'idf_paragraph'] = np.sum([idfs.get(word, 0.0) for word in paragraph])\n",
    "        df.loc[index, 'idf_intersection'] = np.sum([idfs.get(word, 0.0) for word in paragraph & question])\n",
    "    df['relative_question_len'] = df['len_question'] / df['len_paragraph']\n",
    "    df['relative_intersection_len'] = df['len_intersection'] / df['len_paragraph']\n",
    "    df['relative_intersection_question_len'] = df['len_intersection'] / df['len_question']\n",
    "    df['relative_question_idf'] = df['idf_question'] / df['idf_paragraph']\n",
    "    df['relative_intersection_idf'] = df['idf_intersection'] / df['idf_paragraph']\n",
    "    df['relative_intersection_question_idf'] = df['idf_intersection'] / df['idf_question']\n",
    "    df['word_idf_paragraph'] = df['idf_paragraph'] / df['len_paragraph']\n",
    "    df['word_idf_question'] = df['idf_question'] / df['len_question']\n",
    "    df['word_idf_intersection'] = df['idf_intersection'] / df['len_intersection'] \n",
    "    df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemmarize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dftrain complete - paragraph_lemmatized\n",
      "dftrain complete - question_lemmatized\n",
      "dftest complete - paragraph_lemmatized\n",
      "dftest complete - question_lemmatized\n",
      "CPU times: user 2h 45min 55s, sys: 8min 10s, total: 2h 54min 5s\n",
      "Wall time: 2h 57min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train - paragraph\n",
    "df = pd.DataFrame(dftrain.paragraph.unique())\n",
    "df.columns = ['paragraph']\n",
    "df['paragraph_lemmatized'] = df['paragraph'].map(lambda x: preprocess(x))\n",
    "dftrain = dftrain.merge(df, on = 'paragraph', how = 'left')\n",
    "print('dftrain complete - paragraph_lemmatized')\n",
    "\n",
    "# train - question\n",
    "df = pd.DataFrame(dftrain.question.unique())\n",
    "df.columns = ['question']\n",
    "df['question_lemmatized'] = df['question'].map(lambda x: preprocess(x))\n",
    "dftrain = dftrain.merge(df, on = 'question', how = 'left')\n",
    "print('dftrain complete - question_lemmatized')\n",
    "\n",
    "# test - paragraph\n",
    "df = pd.DataFrame(dftest.paragraph.unique())\n",
    "df.columns = ['paragraph']\n",
    "df['paragraph_lemmatized'] = df['paragraph'].map(lambda x: preprocess(x))\n",
    "dftest = dftest.merge(df, on = 'paragraph', how = 'left')\n",
    "print('dftest complete - paragraph_lemmatized')\n",
    "\n",
    "# test - question\n",
    "df = pd.DataFrame(dftest.question.unique())\n",
    "df.columns = ['question']\n",
    "df['question_lemmatized'] = df['question'].map(lambda x: preprocess(x))\n",
    "dftest = dftest.merge(df, on = 'question', how = 'left')\n",
    "print('dftest complete - question_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\"./data/ruwikiruscorpora_0_300_20.bin.gz\", binary=True)\n",
    "new_vocab = {}\n",
    "[new_vocab.update({k[0].replace(\"::\", \"_\").split('_')[0]: k[1]}) for k in w2v.vocab.items()]\n",
    "w2v.vocab = new_vocab\n",
    "\n",
    "# or build model\n",
    "\n",
    "# model = word2vec.Word2Vec(np.append(dftrain.paragraph_lemmatized, \n",
    "#                                     np.append(dftrain[dftrain.generated_p_2 ==0][dftrain.generated_p ==0].question_lemmatized, \n",
    "#                                               dftest.paragraph_lemmatized)),\n",
    "#                  size=100, \n",
    "#                  window=5, \n",
    "#                  min_count=5, \n",
    "#                  workers=4)\n",
    "# fname = '../results/word2vec_model'\n",
    "# model.save(fname)\n",
    "# # model = word2vec.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculating word2vec:  34%|███▍      | 54860/159171 [03:17<06:41, 259.76it/s]"
     ]
    }
   ],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating word2vec\"):  \n",
    "        question = row['question_lemmatized']\n",
    "        paragraph = row['paragraph_lemmatized']\n",
    "        w2v_similarities = []\n",
    "        for question_word in question.split(' '):\n",
    "            max_similarity = 0.0\n",
    "            for paragraph_word in paragraph.split(' '):\n",
    "                if (question_word in model.wv.vocab) and (paragraph_word in model.wv.vocab):\n",
    "                    similarity = model.similarity(question_word, paragraph_word)\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "            w2v_similarities.append(max_similarity)\n",
    "        w2v_similarities = [x for x in w2v_similarities if x > 0.2]\n",
    "        \n",
    "        if len(w2v_similarities) == 0:\n",
    "            df.loc[index, 'word2vec_similarity_max'] = 0\n",
    "            df.loc[index, 'word2vec_similarity_min'] = 0\n",
    "        else:\n",
    "            df.loc[index, 'word2vec_similarity_max'] = np.max(w2v_similarities)\n",
    "            df.loc[index, 'word2vec_similarity_min'] = np.min(w2v_similarities)\n",
    "        df.loc[index, 'word2vec_similarity_mean'] = np.mean(w2v_similarities)\n",
    "        df.loc[index, 'word2vec_similarity_std'] = np.std(w2v_similarities)\n",
    "    df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"build features for \" + name):\n",
    "        intersection = texts_intersection(row.paragraph_lemmatized, row.question_lemmatized, model, 0.2)\n",
    "        df.loc[index, 'len_intersection_w2v'] = len(intersection)\n",
    "        df.loc[index, 'idf_intersection_w2v'] = np.sum([idfs.get(word, 0.0) for word in intersection])\n",
    "    df['relative_intersection_len_w2v'] = df['len_intersection_w2v'] / df['len_paragraph']\n",
    "    df['relative_intersection_question_len_w2v'] = df['len_intersection_w2v'] / df['len_question']\n",
    "    df['relative_intersection_idf_w2v'] = df['idf_intersection_w2v'] / df['idf_paragraph']\n",
    "    df['relative_intersection_question_idf_w2v'] = df['idf_intersection_w2v'] / df['idf_question']\n",
    "    df['word_idf_intersection_w2v'] = df['idf_intersection_w2v'] / df['len_intersection'] \n",
    "    df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word mover distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating word mover distance for \" + name):\n",
    "        df.loc[index, 'word_mover_distance'] = model.wmdistance(row['paragraph'], row['question'])\n",
    "        df.loc[index, 'word_mover_distance_lemm'] = model.wmdistance(row['paragraph_lemmatized'], row['question_lemmatized'])\n",
    "        wmds = []\n",
    "        for paragraph_sentence in text_to_sentence(row['paragraph']):\n",
    "            wmds.append(model.wmdistance(paragraph_sentence, row['question']))\n",
    "        df.loc[index, 'word_mover_sentence_distance_mean'] = np.mean(wmds)\n",
    "        df.loc[index, 'word_mover_sentence_distance_std'] = np.std(wmds)\n",
    "        df.loc[index, 'word_mover_sentence_distance_max'] = np.max(wmds)\n",
    "        df.loc[index, 'word_mover_sentence_distance_min'] = np.min(wmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating word mover distance for \" + name):\n",
    "        question = row.question\n",
    "        question_lemm = row.question_lemmatized\n",
    "        question_len = len(question_lemm)\n",
    "        similarities = []\n",
    "        similarities_lemm = []\n",
    "        if int(question_len / 3) != 0:\n",
    "            for i in range(0, question_len, int(question_len / 3)):\n",
    "                question_chunk = question[i:i + int(question_len / 3)]\n",
    "                similarities.append(model.wmdistance(question_chunk, row['paragraph']))\n",
    "                question_chunk_lemm = question_lemm[i:i + int(question_len / 3)]\n",
    "                similarities_lemm.append(model.wmdistance(question_chunk_lemm, row['paragraph_lemmatized']))\n",
    "        elif int(question_len / 2) != 0:\n",
    "            for i in range(0, question_len, int(question_len / 2)):\n",
    "                question_chunk = question[i:i + int(question_len / 2)]\n",
    "                similarities.append(model.wmdistance(question_chunk, row['paragraph']))\n",
    "                question_chunk_lemm = question_lemm[i:i + int(question_len / 2)]\n",
    "                similarities_lemm.append(model.wmdistance(question_chunk_lemm, row['paragraph_lemmatized']))\n",
    "        else:\n",
    "            question_chunk = question\n",
    "            similarities.append(model.wmdistance(question_chunk, row['paragraph']))\n",
    "            question_chunk_lemm = question_lemm\n",
    "            similarities_lemm.append(model.wmdistance(question_chunk_lemm, row['paragraph_lemmatized']))\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_max'] = np.max(similarities)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_min'] = np.min(similarities)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_mean'] = np.mean(similarities)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_std'] = np.std(similarities)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_lemm_max'] = np.max(similarities_lemm)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_lemm_min'] = np.min(similarities_lemm)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_lemm_mean'] = np.mean(similarities_lemm)\n",
    "        df.loc[index, 'word_mover_sentence_distance_split_lemm_std'] = np.std(similarities_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max match sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating max overlap\"):        \n",
    "        overlaps = get_max_match_sentance(row)\n",
    "        \n",
    "        # vars\n",
    "        a = [len(x) / len(row['paragraph']) for x in overlaps]\n",
    "        b = [len(x) / len(row['question']) for x in overlaps]\n",
    "        c = [np.sum([idfs.get(word, 0.0) for word in overlap]) for overlap in overlaps]\n",
    "        \n",
    "        # max\n",
    "        max_overlap = ''\n",
    "        max_overlap_idf = 0\n",
    "        for overlap in overlaps:\n",
    "            overlap_idf = np.sum([idfs.get(word, 0.0) for word in overlap])\n",
    "            if overlap_idf > max_overlap_idf:\n",
    "                max_overlap_idf = overlap_idf\n",
    "                max_overlap = overlap\n",
    "        df.loc[index, 'answer'] = max_overlap\n",
    "        \n",
    "        df.loc[index, 'len_overlap_relative_paragraph_max'] = np.max(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_max'] = np.max(b)\n",
    "        df.loc[index, 'idf_overlap_max'] = np.max(c)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # means\n",
    "        df.loc[index, 'len_overlap_relative_paragraph_mean'] = np.mean(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_mean'] = np.mean(b)\n",
    "        df.loc[index, 'idf_overlap_mean'] = np.mean(c)\n",
    "        \n",
    "        # stds\n",
    "        df.loc[index, 'len_overlap_relative_paragraph_std'] = np.std(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_std'] = np.std(b)\n",
    "        df.loc[index, 'idf_overlap_std'] = np.std(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating max overlap\"):        \n",
    "        overlaps = get_max_match_sentance(row, w2v=True)\n",
    "        \n",
    "        # vars\n",
    "        a = [len(x) / len(row['paragraph']) for x in overlaps]\n",
    "        b = [len(x) / len(row['question']) for x in overlaps]\n",
    "        c = [np.sum([idfs.get(word, 0.0) for word in overlap]) for overlap in overlaps]\n",
    "        \n",
    "        # max\n",
    "        df.loc[index, 'len_overlap_relative_paragraph_max_w2v'] = np.max(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_max_w2v'] = np.max(b)\n",
    "        df.loc[index, 'idf_overlap_max_w2v'] = np.max(c)\n",
    "        \n",
    "        # max\n",
    "        max_overlap = ''\n",
    "        max_overlap_idf = 0\n",
    "        for overlap in overlaps:\n",
    "            overlap_idf = np.sum([idfs.get(word, 0.0) for word in overlap])\n",
    "            if overlap_idf > max_overlap_idf:\n",
    "                max_overlap_idf = overlap_idf\n",
    "                max_overlap = overlap\n",
    "        df.loc[index, 'answer_w2v'] = max_overlap\n",
    "        \n",
    "        # means\n",
    "        df.loc[index, 'len_overlap_relative_paragraph_mean_w2v'] = np.mean(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_mean_w2v'] = np.mean(b)\n",
    "        df.loc[index, 'idf_overlap_mean_w2v'] = np.mean(c)\n",
    "        \n",
    "        # stds\n",
    "        df.loc[index, 'len_overlap_relative_paragraph_std_w2v'] = np.std(a)\n",
    "        df.loc[index, 'len_overlap_relative_question_std_w2v'] = np.std(b)\n",
    "        df.loc[index, 'idf_overlap_std_w2v'] = np.std(c)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**metric learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://all-umass.github.io/metric-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v -> covariance matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metric_learn import Covariance\n",
    "from metric_learn import LMNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "vectorizer.fit(set(np.append(pd.unique([x for x in dftrain.paragraph.values]),\n",
    "                                     np.append(pd.unique([x for x in dftrain.question.values]),\n",
    "                                              pd.unique([x for x in dftest.paragraph.values])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2022270 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 262 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(dftrain.paragraph.values[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-21405c99026e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcov\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mCovariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\metric_learn\\covariance.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0munused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \"\"\"\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0maweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3064\u001b[1;33m     \u001b[0mavg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3065\u001b[0m     \u001b[0mw_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mscl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m             \u001b[0mscl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "cov  = Covariance().fit(np.matrix(vectorizer.transform(dftrain.paragraph.values[:25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cov' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-74a48992f56f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdftrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cov' is not defined"
     ]
    }
   ],
   "source": [
    "cov.metric(dftrain.paragraph.values[0], dftrain.question.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov.metric('В годовом отчете 1998 г. МВФ впервые заявил, что эффективный банковский надзор должен быть непрерывным.. это может быть обеспечено, главным образом, за счет документарного надзора как на микропруденциальном, так и на макропруденциальном уровне… макропруденциальный анализ основан на исследовании рынка и макроэкономической информации, в фокусе его внимания находятся ключевые рынки активов, финансовые посредники, макроэкономическое развитие и потенциальные дисбалансы . Для макропруденциального анализа в 2000 г. МВФ разработал макропруденциальные индикаторы (macroprudential indicators), которые на следующий год были переименованы в показатели финансовой устойчивости (financial soundness indicators). В 2003 г. программа расчета макропоказателей финансовой устойчивости была внедрена в Банке России.', 'Сколько всего в устойчивости financial soundness indicators?\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov.transform(dftrain.question.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "vectorizer.fit(set(np.append(pd.unique([x for x in dftrain.paragraph_lemmatized.values]),\n",
    "                                     np.append(pd.unique([x for x in dftrain[dftrain.generated_p_2 ==0][dftrain.generated_p ==0].question_lemmatized.values]),\n",
    "                                              pd.unique([x for x in dftest.paragraph_lemmatized.values])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate cummulative nlpS\n",
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating nlps for \" + name):\n",
    "        vectors = vectorizer.transform([row.paragraph_lemmatized, row.question_lemmatized])\n",
    "        df.loc[index, 'similarity_tfid'] = (vectors * vectors.T)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate cummulative nlpS - split question to chunks\n",
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"calculating nlps for \" + name):\n",
    "        question = row.question_lemmatized\n",
    "        question_str = question\n",
    "        question_len = len(question_str)\n",
    "        similarities = []\n",
    "        if int(question_len / 3) != 0:\n",
    "            for i in range(0, question_len, int(question_len / 3)):\n",
    "                question_chunk = question[i:i + int(question_len / 3)]\n",
    "                vectors = vectorizer.transform([row.paragraph_lemmatized, question_chunk])\n",
    "                similarities.append((vectors * vectors.T)[0, 1])\n",
    "        elif int(question_len / 2) != 0:\n",
    "            for i in range(0, question_len, int(question_len / 2)):\n",
    "                question_chunk = question[i:i + int(question_len / 2)]\n",
    "                vectors = vectorizer.transform([row.paragraph_lemmatized, question_chunk])\n",
    "                similarities.append((vectors * vectors.T)[0, 1])\n",
    "        else:\n",
    "            question_chunk = question\n",
    "            vectors = vectorizer.transform([row.paragraph_lemmatized, question_chunk])\n",
    "            similarities.append((vectors * vectors.T)[0, 1])\n",
    "        df.loc[index, 'similarity_tfid_split_max'] = np.max(similarities)\n",
    "        df.loc[index, 'similarity_tfid_split_min'] = np.min(similarities)\n",
    "        df.loc[index, 'similarity_tfid_split_mean'] = np.mean(similarities)\n",
    "        df.loc[index, 'similarity_tfid_split_std'] = np.std(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**словосочетания**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"grammar for \" + name):\n",
    "        question_patterns = list(get_pattern(row.question).keys())\n",
    "        paragraph_patterns = list(get_pattern(row.paragraph).keys())\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "        vectorizer.fit(np.append(question_patterns, paragraph_patterns))\n",
    "        similarity_pattern = []\n",
    "        for pattern in question_patterns:\n",
    "            vectors = vectorizer.transform(np.append(pattern, paragraph_patterns))\n",
    "            # eliminate diagonal 1'ses\n",
    "            matrix = (vectors * vectors.T)\n",
    "            n = matrix.shape[0]\n",
    "            matrix[range(n), range(n)] = 0\n",
    "            similarity_pattern.append(matrix[0].max())\n",
    "        # get max similarity\n",
    "        df.loc[index, 'similarity_pattern_mean'] = np.mean(similarity_pattern)\n",
    "        df.loc[index, 'similarity_pattern_std'] = np.std(similarity_pattern)\n",
    "        if len(similarity_pattern) == 0:\n",
    "            df.loc[index, 'similarity_pattern_max'] = 0\n",
    "            df.loc[index, 'similarity_pattern_min'] = 0\n",
    "        else:\n",
    "            df.loc[index, 'similarity_pattern_max'] = np.max(similarity_pattern)\n",
    "            df.loc[index, 'similarity_pattern_min'] = np.min(similarity_pattern)\n",
    "    df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"grammar for \" + name):\n",
    "        question_lemmatized = row.question_lemmatized\n",
    "        \n",
    "        unique_words_len = len(set(question_lemmatized))\n",
    "        df.loc[index, 'unique_words_relative_to_question'] = unique_words_len / row.len_question\n",
    "        \n",
    "        word_occurences = []\n",
    "        for word in question_lemmatized.split(' '):\n",
    "            word_occurences.append(question_lemmatized.count(word))\n",
    "            \n",
    "        df.loc[index, 'word_ocurences_mean'] = np.mean(word_occurences)\n",
    "        df.loc[index, 'word_ocurences_std'] = np.std(word_occurences)\n",
    "        df.loc[index, 'word_ocurences_max'] = np.max(word_occurences)\n",
    "        df.loc[index, 'word_ocurences_max_relative_to_question'] = np.max(word_occurences) / row.len_question\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**language tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add multiproccessing\n",
    "# https://stackoverflow.com/questions/34512646/how-to-speed-up-api-requests\n",
    "# https://stackoverflow.com/questions/33550312/multiprocessing-writing-to-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/82208 [00:00<?, ?it/s]\n",
      "  0%|                                                                             | 1/82208 [00:01<24:14:46,  1.06s/it]\n",
      "  0%|                                                                             | 2/82208 [00:02<24:30:36,  1.07s/it]\n",
      "  0%|                                                                             | 3/82208 [00:03<24:24:35,  1.07s/it]\n",
      "  0%|                                                                             | 4/82208 [00:04<24:11:23,  1.06s/it]\n",
      "  0%|                                                                             | 5/82208 [00:05<24:03:41,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Подключение не установлено, т.к. конечный компьютер отверг запрос на подключение",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-216fa4834c50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://localhost:8081/v2/check'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'language'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'ru-RU'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matches'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmatches_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     for match in matches:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 141\u001b[1;33m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['question'] = np.append(dftrain.question.unique(), dftest.question.unique())\n",
    "for index, row in tqdm.tqdm(df.iterrows(), total = df.shape[0]):\n",
    "    matches = requests.get('http://localhost:8081/v2/check', params={'text':row.question, 'language':'ru-RU'}).json().get('matches')\n",
    "    matches_dict = {}\n",
    "    for match in matches:\n",
    "        issue_type = match.get('rule').get('issueType')\n",
    "        issue_count = matches_dict.get(issue_type)\n",
    "        \n",
    "        if issue_count is None:\n",
    "            matches_dict.update({issue_type: 1})\n",
    "        else:\n",
    "            matches_dict.update({issue_type: issue_count + 1})\n",
    "            \n",
    "    \n",
    "    for item in matches_dict.items():\n",
    "        df.loc[index, item[0]] = item[1]\n",
    "df.fillna(0, inplace = True)\n",
    "\n",
    "dftrain = dftrain.merge(df, on = 'question', how = 'left')\n",
    "dftest = dftest.merge(df, on = 'question', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим проверку грамматики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'\\', \\''.join(dftrain.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns = ['len_paragraph', 'len_question', 'len_intersection', 'idf_question', 'idf_paragraph', 'idf_intersection', 'relative_question_len', 'relative_intersection_len', 'relative_intersection_question_len', 'relative_question_idf', 'relative_intersection_idf', 'relative_intersection_question_idf', 'word_idf_paragraph', 'word_idf_question', 'word_idf_intersection', 'word2vec_similarity_max', 'word2vec_similarity_min', 'word2vec_similarity_mean', 'word2vec_similarity_std', 'len_intersection_w2v', 'idf_intersection_w2v', 'relative_intersection_len_w2v', 'relative_intersection_question_len_w2v', 'relative_intersection_idf_w2v', 'relative_intersection_question_idf_w2v', 'word_idf_intersection_w2v', 'word_mover_distance', 'word_mover_distance_lemm', 'word_mover_sentence_distance_mean', 'word_mover_sentence_distance_std', 'word_mover_sentence_distance_max', 'word_mover_sentence_distance_min', 'word_mover_sentence_distance_split_max', 'word_mover_sentence_distance_split_min', 'word_mover_sentence_distance_split_mean', 'word_mover_sentence_distance_split_std', 'word_mover_sentence_distance_split_lemm_max', 'word_mover_sentence_distance_split_lemm_min', 'word_mover_sentence_distance_split_lemm_mean', 'word_mover_sentence_distance_split_lemm_std', 'len_overlap_relative_paragraph_max', 'len_overlap_relative_question_max', 'idf_overlap_max', 'len_overlap_relative_paragraph_mean', 'len_overlap_relative_question_mean', 'idf_overlap_mean', 'len_overlap_relative_paragraph_std', 'len_overlap_relative_question_std', 'idf_overlap_std', 'len_overlap_relative_paragraph_max_w2v', 'len_overlap_relative_question_max_w2v', 'idf_overlap_max_w2v', 'len_overlap_relative_paragraph_mean_w2v', 'len_overlap_relative_question_mean_w2v', 'idf_overlap_mean_w2v', 'len_overlap_relative_paragraph_std_w2v', 'len_overlap_relative_question_std_w2v', 'idf_overlap_std_w2v', 'similarity_tfid', 'similarity_tfid_split_max', 'similarity_tfid_split_min', 'similarity_tfid_split_mean', 'similarity_tfid_split_std', 'similarity_pattern_mean', 'similarity_pattern_std', 'similarity_pattern_max', 'similarity_pattern_min', 'unique_words_relative_to_question', 'word_ocurences_mean', 'word_ocurences_std', 'word_ocurences_max', 'word_ocurences_max_relative_to_question']\n",
    "\n",
    "# dftrainfull = xgb.DMatrix(dftrain[dftrain.generated_p == 0][dftrain.generated_p_2 == 0][columns], dftrain[dftrain.generated_p == 0][dftrain.generated_p_2 == 0].generated_p_3)\n",
    "# # dftrainfull = xgb.DMatrix(dftrain[columns], dftrain[['generated_p', 'generated_p_2', 'generated_p_3']].sum(axis=1))\n",
    "# params = {}\n",
    "# params['objective'] = 'binary:logistic'\n",
    "\n",
    "\n",
    "# # triple xgb\n",
    "# a = []\n",
    "# b = []\n",
    "# for i in range(0, 3):\n",
    "#     params_i = params\n",
    "#     params_i['colsample_bylevel'] = 0.5 + i * 0.25\n",
    "#     params_i['colsample_bytree'] = 0.5 + i * 0.25\n",
    "#     bst = xgb.train(params, dtrainfull, num_boost_round=100)\n",
    "#     a.append(bst.predict(xgb.DMatrix(dftrain[columns])))\n",
    "#     b.append(bst.predict(xgb.DMatrix(dftest[columns])))\n",
    "\n",
    "# dftrain['grammar_prediction'] = [sum(e)/len(e) for e in zip(*a)]\n",
    "# dftest['grammar_prediction'] = [sum(e)/len(e) for e in zip(*b)]\n",
    "\n",
    "# score = roc_auc_score(dftrain.generated_p_3, dftrain['grammar_prediction'])\n",
    "# print('e - roc_auc_score: ' + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>grammar_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В какой мифологии два ворона Хугин и Мунин шептали новости в уши бога Одина?</td>\n",
       "      <td>0.001077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Когда инсайдер покупает или гипотезы чего эволюционно устойчивых стратегий развития стратегий войн, развития стратегий войн, развития стратегий?</td>\n",
       "      <td>0.881770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Какие предположения, по мнению Ньютона, допустимы в натуральной философии (то есть физике)?</td>\n",
       "      <td>0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В каких условиях метаболизм поддерживает порядок за счёт создания беспорядка?</td>\n",
       "      <td>0.000652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Каким мелодиям Прокофьева особенно не укладывается в таблицы именно этих языков, алфавит которых укладывается в места, недоступные для мощности в существенной мере способствовал систематизации его чувства, опыт?</td>\n",
       "      <td>0.878052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Какая песня стала гимном панк-движения?</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Каким методом модель ЦП доводится до ума?</td>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>При какой рекомбинации у высших грибов диплоидное (зиготическое) ядро без периода покоя делится редукционно с образованием тетрады?</td>\n",
       "      <td>0.001726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Когда деяние совершается в соответствующую уголовно правовую оценку общественной опасностью и законодатель сочтёт нужным создать соответствующую уголовно правовую норму заработной платы и заражения вирусами вызываются такие заболевания, как настоящий щит?</td>\n",
       "      <td>0.952995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Где может исполняться программа на языке третьего поколения?</td>\n",
       "      <td>0.001266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>В какой энциклопедии в 1939 году была опубликована статья о Цицероне?</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Какие языки программирования позволяют воплощать в виде кода алгоритмы, но не архитектуру программ?</td>\n",
       "      <td>0.043414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Как называются виды снежных осадков, которые, из-за циклов таяния и замораживания, падают в виде шариков, а не хлопьев?</td>\n",
       "      <td>0.015259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Какой термин предполагает, что семантическая модель языка в большей степени учитывает особенности мышления человека, нежели машины?</td>\n",
       "      <td>0.002317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Кто может сокращаться до ie io ba ny ia категория культовые сооружения, построенные в родах hypoplectrus и воплощать их в условном пространстве?</td>\n",
       "      <td>0.041966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Когда ввели различие между естественными и социальными условиями от направлений развивали теоретические обоснования отличия мира природы от реальности, допустил несколько направлений?</td>\n",
       "      <td>0.356292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Сколько уровней обозначаются l1, l2 и делаются попытки её драматизировать своё отношение к объёму?</td>\n",
       "      <td>0.127443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>В России при согласии обоих супругов, не имеющих общих детей, развод возможен где?</td>\n",
       "      <td>0.001761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Какой термин означает, что даже если программа содержит некий баг (в частности, логическую ошибку), она тем не менее не способна нарушить целостность данных и обрушиться?</td>\n",
       "      <td>0.024450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Как фэнтези и фэнтези с фэнтези как фэнтези и правила, противоречащие моим утверждениям, вы не игровые вселенные?</td>\n",
       "      <td>0.755333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Когда эндлихер был чересчур кратко, а ощутимым недостатком работы?</td>\n",
       "      <td>0.004346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Когда чья либо территории, объекта, явления или размеры дворца относительно небольшие, но глубокие овраги?</td>\n",
       "      <td>0.047498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Какие исследования показали, что ферменты в метаболическом пути имеют общее происхождение?</td>\n",
       "      <td>0.002328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>От чего зависит дееспособность лица?</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Как свидетельствует пример заявки на патент, а разрыв между европейским лидером в пятьсот?</td>\n",
       "      <td>0.118909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Что в своем творчестве сочетали многие панк-группы?</td>\n",
       "      <td>0.003546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Кто издаёт указы согласно конституции боснии и распространена по миру и самок, поддерживаются гены альтруизма поддерживаются?</td>\n",
       "      <td>0.109609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Чем питаются воробьинообразные?</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Увеличение сложности связей в экосистеме с чем связывают сейчас?</td>\n",
       "      <td>0.002645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>В каком месяце появляется наибольшее число видов жуков в средней полосе России?</td>\n",
       "      <td>0.003752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>Какие ученые исследуют муравьёв как в лабораторных, так и в полевых условиях?</td>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>Кого достаёт из-под коры новокаледонский ворон с помощью самодельных орудий труда?</td>\n",
       "      <td>0.000917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>Когда спустя и проведении традиционных региональных встреч членов недавно созданной?</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74259</th>\n",
       "      <td>Чем, в основном, питаются такие хищные птицы, как сапсан, ястребы и челнок?</td>\n",
       "      <td>0.000579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74260</th>\n",
       "      <td>Сколько зрителей вмещал стадион Сан Сиро после реконструкции?</td>\n",
       "      <td>0.008834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74261</th>\n",
       "      <td>Что совершено на продаже готового бизнеса, агент сделок купли продажи бизнеса, что совершено на полях ливонской хроники?</td>\n",
       "      <td>0.509167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74262</th>\n",
       "      <td>Чем то, отличным от рассуждение , чувственного и правильном мышлении , чувственного и формах, и формах, и формах, и формах, и формах, и формах, и формах, и формах, и правильном мышлении?</td>\n",
       "      <td>0.998340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74263</th>\n",
       "      <td>В каком месяце была опубликована одна из первых книг о панк-роке: The Boy Looked at Johnny , Джули Бёрчилл и Тони Парсонс?</td>\n",
       "      <td>0.001712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74264</th>\n",
       "      <td>какой этап обходится в глиоксилатновом цикле у растений и бактерий?</td>\n",
       "      <td>0.001036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74265</th>\n",
       "      <td>Какой язык объединил в себе черты объектно-ориентированного и системного программирования?</td>\n",
       "      <td>0.005184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74266</th>\n",
       "      <td>На почве чего возникали разногласия между Фицроем и Дарвином?</td>\n",
       "      <td>0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74267</th>\n",
       "      <td>Что ранее использовались для доказательств для эффективности для терапии для ряда научных медицинских научных доказательств эффективности подобной терапии для доказательств?</td>\n",
       "      <td>0.713583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74268</th>\n",
       "      <td>Какой термин означает, что даже если программа содержит некий баг (в частности, логическую ошибку), она тем не менее не способна нарушить целостность данных и обрушиться?</td>\n",
       "      <td>0.143790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74269</th>\n",
       "      <td>В каком году Манчестер Юнайтед играла в полосатых сине-белых футболках?</td>\n",
       "      <td>0.011016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74270</th>\n",
       "      <td>Кто живет на поверхности тела муравья и производит особые вещества, которые убивают грибы-сорняки рода Escovopsis?</td>\n",
       "      <td>0.042512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74271</th>\n",
       "      <td>Какие языки размывают границы между интерпретацией и компиляцией?</td>\n",
       "      <td>0.002316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74272</th>\n",
       "      <td>Как он сейчас известен, базировался на волне бума молодежного движения модов и гранжа?</td>\n",
       "      <td>0.017615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74273</th>\n",
       "      <td>Сколько национальных и водной растительностью, попутно поедая водных животных происходит смена времён года на водной основе с водной средой проявляется в специализированной школе?</td>\n",
       "      <td>0.954805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74274</th>\n",
       "      <td>В речи за Архия Цицерон обосновывает пользу литературы для кого?</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74275</th>\n",
       "      <td>Какой храм находится на Гардинер-стрит в Дублине?</td>\n",
       "      <td>0.003792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74276</th>\n",
       "      <td>При каком впрыске головки цилиндров силового агрегата хотя это было?</td>\n",
       "      <td>0.017280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74277</th>\n",
       "      <td>В чем обнаруживается сильное влияние Цицерона на Лактанция?</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74278</th>\n",
       "      <td>Кто вам мешает или встроенный топливный насос высокого давления, устройство, которое позволяет пользователям или встроенный топливный насос 9 дизельный переулок екатеринбург?</td>\n",
       "      <td>0.346470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74279</th>\n",
       "      <td>Сколько это будто данте будто бы связать текущее состояние античной науки?</td>\n",
       "      <td>0.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74280</th>\n",
       "      <td>При чём настоящий яйцеклад у некоторых групп (например, у многих жуков) возникает вторичный, телескопический яйцеклад из сильно уменьшенных в диаметре вершинных сегментов брюшка.</td>\n",
       "      <td>0.014980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74281</th>\n",
       "      <td>В какой стране популярны поединки между самцами жуков, относящихся как к одному, так и разным видам?</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74282</th>\n",
       "      <td>Какие явления, природа которых ещё не выяснена, исследуются и применяются сейчас?</td>\n",
       "      <td>0.016757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74283</th>\n",
       "      <td>Какое топливо нелетучее (то есть, сравнительно плохо испаряется и в замкнутом моторном отделении не образует большого количества легковоспламеняющихся паров)?</td>\n",
       "      <td>0.011906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74284</th>\n",
       "      <td>Какой вид деятельности в Российской Федерации для иностранных граждан и лиц без гражданства осуществляется наравне с гражданами РФ</td>\n",
       "      <td>0.003164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74285</th>\n",
       "      <td>Сколько газет выходило в заметных количествах это углеводороды нс или созданной другими людьми?</td>\n",
       "      <td>0.039195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74286 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              question  \\\n",
       "0                                                                                                                                                                                         В какой мифологии два ворона Хугин и Мунин шептали новости в уши бога Одина?   \n",
       "1                                                                                                                     Когда инсайдер покупает или гипотезы чего эволюционно устойчивых стратегий развития стратегий войн, развития стратегий войн, развития стратегий?   \n",
       "2                                                                                                                                                                          Какие предположения, по мнению Ньютона, допустимы в натуральной философии (то есть физике)?   \n",
       "3                                                                                                                                                                                        В каких условиях метаболизм поддерживает порядок за счёт создания беспорядка?   \n",
       "4                                                  Каким мелодиям Прокофьева особенно не укладывается в таблицы именно этих языков, алфавит которых укладывается в места, недоступные для мощности в существенной мере способствовал систематизации его чувства, опыт?   \n",
       "5                                                                                                                                                                                                                              Какая песня стала гимном панк-движения?   \n",
       "6                                                                                                                                                                                                                            Каким методом модель ЦП доводится до ума?   \n",
       "7                                                                                                                                  При какой рекомбинации у высших грибов диплоидное (зиготическое) ядро без периода покоя делится редукционно с образованием тетрады?   \n",
       "8      Когда деяние совершается в соответствующую уголовно правовую оценку общественной опасностью и законодатель сочтёт нужным создать соответствующую уголовно правовую норму заработной платы и заражения вирусами вызываются такие заболевания, как настоящий щит?   \n",
       "9                                                                                                                                                                                                         Где может исполняться программа на языке третьего поколения?   \n",
       "10                                                                                                                                                                                               В какой энциклопедии в 1939 году была опубликована статья о Цицероне?   \n",
       "11                                                                                                                                                                 Какие языки программирования позволяют воплощать в виде кода алгоритмы, но не архитектуру программ?   \n",
       "12                                                                                                                                             Как называются виды снежных осадков, которые, из-за циклов таяния и замораживания, падают в виде шариков, а не хлопьев?   \n",
       "13                                                                                                                                 Какой термин предполагает, что семантическая модель языка в большей степени учитывает особенности мышления человека, нежели машины?   \n",
       "14                                                                                                                    Кто может сокращаться до ie io ba ny ia категория культовые сооружения, построенные в родах hypoplectrus и воплощать их в условном пространстве?   \n",
       "15                                                                             Когда ввели различие между естественными и социальными условиями от направлений развивали теоретические обоснования отличия мира природы от реальности, допустил несколько направлений?   \n",
       "16                                                                                                                                                                  Сколько уровней обозначаются l1, l2 и делаются попытки её драматизировать своё отношение к объёму?   \n",
       "17                                                                                                                                                                                  В России при согласии обоих супругов, не имеющих общих детей, развод возможен где?   \n",
       "18                                                                                          Какой термин означает, что даже если программа содержит некий баг (в частности, логическую ошибку), она тем не менее не способна нарушить целостность данных и обрушиться?   \n",
       "19                                                                                                                                                   Как фэнтези и фэнтези с фэнтези как фэнтези и правила, противоречащие моим утверждениям, вы не игровые вселенные?   \n",
       "20                                                                                                                                                                                                  Когда эндлихер был чересчур кратко, а ощутимым недостатком работы?   \n",
       "21                                                                                                                                                          Когда чья либо территории, объекта, явления или размеры дворца относительно небольшие, но глубокие овраги?   \n",
       "22                                                                                                                                                                          Какие исследования показали, что ферменты в метаболическом пути имеют общее происхождение?   \n",
       "23                                                                                                                                                                                                                                От чего зависит дееспособность лица?   \n",
       "24                                                                                                                                                                          Как свидетельствует пример заявки на патент, а разрыв между европейским лидером в пятьсот?   \n",
       "25                                                                                                                                                                                                                 Что в своем творчестве сочетали многие панк-группы?   \n",
       "26                                                                                                                                       Кто издаёт указы согласно конституции боснии и распространена по миру и самок, поддерживаются гены альтруизма поддерживаются?   \n",
       "27                                                                                                                                                                                                                                     Чем питаются воробьинообразные?   \n",
       "28                                                                                                                                                                                                    Увеличение сложности связей в экосистеме с чем связывают сейчас?   \n",
       "29                                                                                                                                                                                     В каком месяце появляется наибольшее число видов жуков в средней полосе России?   \n",
       "...                                                                                                                                                                                                                                                                ...   \n",
       "74256                                                                                                                                                                                    Какие ученые исследуют муравьёв как в лабораторных, так и в полевых условиях?   \n",
       "74257                                                                                                                                                                               Кого достаёт из-под коры новокаледонский ворон с помощью самодельных орудий труда?   \n",
       "74258                                                                                                                                                                             Когда спустя и проведении традиционных региональных встреч членов недавно созданной?   \n",
       "74259                                                                                                                                                                                      Чем, в основном, питаются такие хищные птицы, как сапсан, ястребы и челнок?   \n",
       "74260                                                                                                                                                                                                    Сколько зрителей вмещал стадион Сан Сиро после реконструкции?   \n",
       "74261                                                                                                                                         Что совершено на продаже готового бизнеса, агент сделок купли продажи бизнеса, что совершено на полях ливонской хроники?   \n",
       "74262                                                                       Чем то, отличным от рассуждение , чувственного и правильном мышлении , чувственного и формах, и формах, и формах, и формах, и формах, и формах, и формах, и формах, и правильном мышлении?   \n",
       "74263                                                                                                                                       В каком месяце была опубликована одна из первых книг о панк-роке: The Boy Looked at Johnny , Джули Бёрчилл и Тони Парсонс?   \n",
       "74264                                                                                                                                                                                              какой этап обходится в глиоксилатновом цикле у растений и бактерий?   \n",
       "74265                                                                                                                                                                       Какой язык объединил в себе черты объектно-ориентированного и системного программирования?   \n",
       "74266                                                                                                                                                                                                    На почве чего возникали разногласия между Фицроем и Дарвином?   \n",
       "74267                                                                                    Что ранее использовались для доказательств для эффективности для терапии для ряда научных медицинских научных доказательств эффективности подобной терапии для доказательств?   \n",
       "74268                                                                                       Какой термин означает, что даже если программа содержит некий баг (в частности, логическую ошибку), она тем не менее не способна нарушить целостность данных и обрушиться?   \n",
       "74269                                                                                                                                                                                          В каком году Манчестер Юнайтед играла в полосатых сине-белых футболках?   \n",
       "74270                                                                                                                                               Кто живет на поверхности тела муравья и производит особые вещества, которые убивают грибы-сорняки рода Escovopsis?   \n",
       "74271                                                                                                                                                                                                Какие языки размывают границы между интерпретацией и компиляцией?   \n",
       "74272                                                                                                                                                                           Как он сейчас известен, базировался на волне бума молодежного движения модов и гранжа?   \n",
       "74273                                                                              Сколько национальных и водной растительностью, попутно поедая водных животных происходит смена времён года на водной основе с водной средой проявляется в специализированной школе?   \n",
       "74274                                                                                                                                                                                                 В речи за Архия Цицерон обосновывает пользу литературы для кого?   \n",
       "74275                                                                                                                                                                                                                Какой храм находится на Гардинер-стрит в Дублине?   \n",
       "74276                                                                                                                                                                                             При каком впрыске головки цилиндров силового агрегата хотя это было?   \n",
       "74277                                                                                                                                                                                                      В чем обнаруживается сильное влияние Цицерона на Лактанция?   \n",
       "74278                                                                                   Кто вам мешает или встроенный топливный насос высокого давления, устройство, которое позволяет пользователям или встроенный топливный насос 9 дизельный переулок екатеринбург?   \n",
       "74279                                                                                                                                                                                       Сколько это будто данте будто бы связать текущее состояние античной науки?   \n",
       "74280                                                                               При чём настоящий яйцеклад у некоторых групп (например, у многих жуков) возникает вторичный, телескопический яйцеклад из сильно уменьшенных в диаметре вершинных сегментов брюшка.   \n",
       "74281                                                                                                                                                             В какой стране популярны поединки между самцами жуков, относящихся как к одному, так и разным видам?   \n",
       "74282                                                                                                                                                                                Какие явления, природа которых ещё не выяснена, исследуются и применяются сейчас?   \n",
       "74283                                                                                                   Какое топливо нелетучее (то есть, сравнительно плохо испаряется и в замкнутом моторном отделении не образует большого количества легковоспламеняющихся паров)?   \n",
       "74284                                                                                                                               Какой вид деятельности в Российской Федерации для иностранных граждан и лиц без гражданства осуществляется наравне с гражданами РФ   \n",
       "74285                                                                                                                                                                  Сколько газет выходило в заметных количествах это углеводороды нс или созданной другими людьми?   \n",
       "\n",
       "       grammar_prediction  \n",
       "0                0.001077  \n",
       "1                0.881770  \n",
       "2                0.000781  \n",
       "3                0.000652  \n",
       "4                0.878052  \n",
       "5                0.002892  \n",
       "6                0.002890  \n",
       "7                0.001726  \n",
       "8                0.952995  \n",
       "9                0.001266  \n",
       "10               0.000483  \n",
       "11               0.043414  \n",
       "12               0.015259  \n",
       "13               0.002317  \n",
       "14               0.041966  \n",
       "15               0.356292  \n",
       "16               0.127443  \n",
       "17               0.001761  \n",
       "18               0.024450  \n",
       "19               0.755333  \n",
       "20               0.004346  \n",
       "21               0.047498  \n",
       "22               0.002328  \n",
       "23               0.000141  \n",
       "24               0.118909  \n",
       "25               0.003546  \n",
       "26               0.109609  \n",
       "27               0.000559  \n",
       "28               0.002645  \n",
       "29               0.003752  \n",
       "...                   ...  \n",
       "74256            0.001472  \n",
       "74257            0.000917  \n",
       "74258            0.007667  \n",
       "74259            0.000579  \n",
       "74260            0.008834  \n",
       "74261            0.509167  \n",
       "74262            0.998340  \n",
       "74263            0.001712  \n",
       "74264            0.001036  \n",
       "74265            0.005184  \n",
       "74266            0.001206  \n",
       "74267            0.713583  \n",
       "74268            0.143790  \n",
       "74269            0.011016  \n",
       "74270            0.042512  \n",
       "74271            0.002316  \n",
       "74272            0.017615  \n",
       "74273            0.954805  \n",
       "74274            0.001600  \n",
       "74275            0.003792  \n",
       "74276            0.017280  \n",
       "74277            0.000115  \n",
       "74278            0.346470  \n",
       "74279            0.008012  \n",
       "74280            0.014980  \n",
       "74281            0.000999  \n",
       "74282            0.016757  \n",
       "74283            0.011906  \n",
       "74284            0.003164  \n",
       "74285            0.039195  \n",
       "\n",
       "[74286 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.options.display.max_colwidth = 10000\n",
    "# dftest[['question', 'grammar_prediction']]#[dftest.grammar_prediction < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка типа вопроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_types_keys = ['кто', 'кого', 'когда', 'почему', 'зачем', 'отчего', 'куда', 'кому', 'чему', 'как', 'о ком', 'чем',\n",
    "                  'на чем', 'как', 'сколько', 'где', 'какого', 'откуда', 'чего', 'кем', 'чем', 'чей', 'который', 'какой',\n",
    "                  'какая', 'какие', 'какое', 'каких', 'какие', 'чья', 'чьи', 'чье']\n",
    "def update_question_types_dict(question, answer_patterns):\n",
    "    \n",
    "    '''\n",
    "    question - text\n",
    "    answer_patterns - patterns of question (intersection) paragraph\n",
    "    \n",
    "    function remebers what patterns correlate with question type\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    for word in question.split(' '):\n",
    "        if word in question_types_keys:\n",
    "            \n",
    "            patterns_dict = question_types_dict.get(word)\n",
    "            \n",
    "            if patterns_dict is None:\n",
    "                patterns_dict = {}\n",
    "            \n",
    "            for answer_pattern in answer_patterns:\n",
    "                answer_pattern_count = patterns_dict.get(answer_pattern)\n",
    "                \n",
    "                if answer_pattern_count is None:\n",
    "                    answer_pattern_count = 0\n",
    "                    \n",
    "                patterns_dict.update({answer_pattern:answer_pattern_count+1})\n",
    "            \n",
    "            question_types_dict.update({word:patterns_dict})\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45 калибр , 450 калибр где чаще употребляется это сочетание?\n"
     ]
    }
   ],
   "source": [
    "# заполним словарь\n",
    "question_types_dict = {}\n",
    "for group_name, group in dftrain[dftrain.target == 1].groupby(['question', 'answer']):\n",
    "    update_question_types_dict(group_name[0], group_name[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# рассчитаем, насколько ответ для вопроса схож со средними значениями \n",
    "for name, df in [('train', dftrain), ('test', dftest)]:\n",
    "    for index, row in tqdm.tqdm(df.groupby(['question', 'answer']), desc=\"q type for \" + name):\n",
    "        question = group_name[0]\n",
    "        answer = group_name[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_types_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftrain.to_csv('../data/train_task1_latest_pred.csv', sep=';', index= False)\n",
    "dftest.to_csv('../data/test_task1_latest_pred.csv', sep=';', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим даные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dftrain = pd.read_csv('../data/train_task1_latest_pred.csv', sep=';')\n",
    "# dftest = pd.read_csv('../data/test_task1_latest_pred.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(dftrain.question_lemmatized.unique())\n",
    "df.columns = ['question_lemmatized']\n",
    "df['question_patterns'] = df['question_lemmatized'].map(lambda x: get_patterns(x.strip('?')))\n",
    "dftrain = dftrain.merge(df, on = 'question_lemmatized', how = 'left')\n",
    "\n",
    "df = pd.DataFrame(dftest.question_lemmatized.unique())\n",
    "df.columns = ['question_lemmatized']\n",
    "df['question_patterns'] = df['question_lemmatized'].map(lambda x: get_patterns(x.strip('?')))\n",
    "dftest = dftest.merge(df, on = 'question_lemmatized', how = 'left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q_patterns = []\n",
    "for question_pattern in dftrain.question_patterns:\n",
    "    q_patterns = set(np.append(list(q_patterns), question_pattern))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "patterns_dictionary = {(v, k) for k,v in enumerate(list([x for x in q_patterns if x is not None]))}\n",
    "patterns_dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### NLTK cfg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.word_tokenize?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = \"Что в творчестве сочетали многие панк-группы?\"\n",
    "text = nltk.word_tokenize(text.lower().translate(remove_punctuation_map))\n",
    "tagged_text = nltk.pos_tag(text, lang='rus')\n",
    "pos_tags = [pos for (token,pos) in nltk.pos_tag(text, lang='rus')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tagged_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "' -> '.join(pos_tags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grammar = CFG.fromstring(\"\"\"\n",
    "  CONJ -> PR\n",
    "  S -> V\n",
    "  V -> A-PRO=pl\n",
    "  A-PRO=pl -> S\n",
    "  S-PRO -> 'что'  \n",
    "  A-PRO=pl -> 'многие'\n",
    "  PR -> 'в'\n",
    "  S -> 'творчестве' | 'панкгруппы'\n",
    "  V -> 'сочетали'\n",
    "  \"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tree = parser.parse(text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
