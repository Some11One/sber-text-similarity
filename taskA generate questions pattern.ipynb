{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import functools\n",
    "import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "import string, nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pymorphy2\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import functools\n",
    "import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk, string\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pymorphy2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(text):\n",
    "    return ' '.join(re.findall(r\"\\w+\", text)).lower()\n",
    "\n",
    "def sentence_to_word(sentences):\n",
    "    sentences_in_words = list()\n",
    "    for sentence in sentences:\n",
    "        sentences_in_words.append(normalize_answer(sentence).split())\n",
    "    return sentences_in_words\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    sentences = text.split(\". \")\n",
    "    return [s.strip() for s in sentences if s.strip() != '']\n",
    "\n",
    "def get_max_match_sentance(data_row):\n",
    "    sentences = text_to_sentence(data_row[\"paragraph\"])\n",
    "    sentences_in_words = sentence_to_word(sentences)\n",
    "    question_in_words = sentence_to_word([data_row[\"question\"]])[0]\n",
    "\n",
    "    max_overlap = None\n",
    "    max_match_sentance_id = None\n",
    "\n",
    "    question_words = set(question_in_words)\n",
    "    for sentance_id in range(len(sentences_in_words)):\n",
    "        sentence_words = set(sentences_in_words[sentance_id])\n",
    "        overlap = len(sentence_words.intersection(question_words))\n",
    "        if max_overlap is None or overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            max_match_sentance_id = sentance_id\n",
    "            \n",
    "            \n",
    "    # do smth to extract answer\n",
    "    return sentences[max_match_sentance_id], ' '.join(list(set(normalize_answer(sentences[max_match_sentance_id]).split()) - set(question_in_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train_task1_latest.csv\")\n",
    "data_test = pd.read_csv(\"data/sdsj_A_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate questions using patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**p4 - герерирование вопроса заполнением случайными словами структуры вопроса\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_patterns(text):\n",
    "    tokens = text.split(' ')\n",
    "    return [[p.tag.POS for p in morph.parse(word)][0] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREP ADJF NOUN ADJF NOUN VERB PREP NOUN PREP NOUN ADJF CONJ NOUN NOUN NOUN CONJ PRCL NOUN'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим массив возможных вариаций частей речи в вопросе\n",
    "df = pd.DataFrame(data.question.unique())\n",
    "df.columns = ['question']\n",
    "df['question_patterns'] = df['question'].map(lambda x: get_patterns(x.strip('?')))\n",
    "data = data.merge(df, on = 'question', how = 'left')\n",
    "data['question_patterns_str'] = data.question_patterns.map(lambda x: ' '.join([i for i in x if i is not None]))\n",
    "unique_question_patterns = data.question_patterns_str.unique()\n",
    "unique_question_patterns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(text, patterns):\n",
    "\n",
    "    '''\n",
    "    Tries to find pattern ([NOUN, VERB] ex.) in text and extract it. None if none found2\n",
    "    '''\n",
    "    \n",
    "    text_patterns = get_patterns(text.strip('?'))\n",
    "    text_patterns_str = ' '.join([i if i is not None else 'NONE' for i in get_patterns(text.strip('?'))])\n",
    "    \n",
    "    i_start = text_patterns_str.find(patterns)\n",
    "    \n",
    "    if i_start == -1:\n",
    "        return None\n",
    "    \n",
    "    # count number of spaces before i_start -> number of words to skip\n",
    "    word_start = text_patterns_str[:i_start].count(' ')\n",
    "    wort_end = word_start + len(patterns.split(' '))\n",
    "    \n",
    "    return ' '.join(text.split(' ')[word_start:wort_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paragraph_id', 'question_id', 'paragraph', 'question', 'target',\n",
       "       'question_patterns', 'question_patterns_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of   0%|▏                                                                            | 21/9078 [02:59<21:31:21,  8.55s/it]>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 891, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 1110, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 481, in _decr_instances\n",
      "    if inst.pos > instance.pos:\n",
      "AttributeError: 'tqdm' object has no attribute 'pos'\n",
      "Exception in thread Thread-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 147, in run\n",
      "    if instance.miniters > 1 and \\\n",
      "AttributeError: 'tqdm' object has no attribute 'miniters'\n",
      "\n",
      "  8%|█████▉                                                                    | 735/9078 [1:07:04<12:41:18,  5.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-832d54152367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;31m# try to find patterns in paragraph / questions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms_p\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[0mfound_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NONE'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatterns_to_find\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfound_text\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-136-a8afd5736d5f>\u001b[0m in \u001b[0;36mfind_patterns\u001b[1;34m(text, patterns)\u001b[0m\n\u001b[0;32m      5\u001b[0m     '''\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtext_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtext_patterns_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NONE'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-ff4a0d7c1e5d>\u001b[0m in \u001b[0;36mget_patterns\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmorph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-ff4a0d7c1e5d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmorph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymorphy2\\analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymorphy2\\units\\by_lookup.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mnormal_forms_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mpara_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_lower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mee\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfixed_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparses\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpara_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dawg_python\\dawgs.py\u001b[0m in \u001b[0;36msimilar_items\u001b[1;34m(self, key, replaces)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \"\"\"\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_similar_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mROOT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dawg_python\\dawgs.py\u001b[0m in \u001b[0;36m_similar_items\u001b[1;34m(self, current_prefix, key, index, replace_chars)\u001b[0m\n\u001b[0;32m    343\u001b[0m                     \u001b[0mres\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mextra_items\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfollow_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dawg_python\\wrapper.py\u001b[0m in \u001b[0;36mfollow_bytes\u001b[1;34m(self, s, index)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m\"Follows transitions.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfollow_char\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_from_byte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dawg_python\\wrapper.py\u001b[0m in \u001b[0;36mfollow_char\u001b[1;34m(self, label, index)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfollow_char\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;34m\"Follows a transition\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m^\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m^\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPRECISION_MASK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\dawg_python\\units.py\u001b[0m in \u001b[0;36moffset\u001b[1;34m(base)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;34m\"\"\" Read an offset to child units from a non-leaf unit. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mEXTENSION_BIT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mPRECISION_MASK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_generated = pd.DataFrame(columns=['paragraph_id', 'paragraph'])\n",
    "data_generated['generated_question'] = ''\n",
    "\n",
    "count = 0\n",
    "for paragraph, paragraph_group in tqdm.tqdm(data.groupby('paragraph')):\n",
    "    sententence_pool = np.append(paragraph_group.question.unique(), paragraph.split('. '))\n",
    "    sententence_pool_2 = np.append(paragraph.split('. '), paragraph_group.question.unique())\n",
    "    \n",
    "    for i in range(0, np.random.randint(1, 5)):\n",
    "    \n",
    "        random_question_patterns = unique_question_patterns[np.random.randint(0, len(unique_question_patterns))]\n",
    "\n",
    "        generated_question = ''\n",
    "        found_patterns = []\n",
    "        patterns = random_question_patterns.split(' ')\n",
    "        first = True\n",
    "        for i in range(len(patterns)):\n",
    "\n",
    "            if len(found_patterns) >= len(patterns[:i + 1]):\n",
    "                continue\n",
    "\n",
    "            failed = 0\n",
    "            patterns_to_find = patterns[i:]\n",
    "            while (len(patterns_to_find) != 0):\n",
    "\n",
    "                # if first - use questions first, else use paragraph first\n",
    "                if first:\n",
    "                    first = False\n",
    "                    s_p = sententence_pool\n",
    "                else:\n",
    "                    s_p = sententence_pool_2\n",
    "\n",
    "                # try to find patterns in paragraph / questions\n",
    "                for sentence in s_p:\n",
    "                    found_text = find_patterns(sentence,  ' '.join([i if i is not None else 'NONE' for i in patterns_to_find]))\n",
    "                    if found_text is not None:\n",
    "                        break\n",
    "\n",
    "                # remember text if found\n",
    "                if found_text is None:\n",
    "                    failed += 1\n",
    "                    patterns_to_find = patterns_to_find[:-1]\n",
    "                    continue\n",
    "                else:\n",
    "                    generated_question += found_text + ' '\n",
    "                    found_patterns.extend(patterns_to_find)\n",
    "                    break\n",
    "\n",
    "\n",
    "        generated_question += '?'\n",
    "        data_generated.loc[count] = [paragraph_group.paragraph_id.values[0], paragraph, generated_question]\n",
    "        count += 1\n",
    "\n",
    "    if count >= 10000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_generated = data_generated[data_generated['generated_question'] != '']\n",
    "data_generated[['paragraph_id', 'paragraph', 'generated_question']].drop_duplicates().dropna().to_csv('data/taskA_generated_questions_p_4.csv', sep=';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** p3 - генерирование повторяющейся стуктуры** - 95.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9078 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/9078 [00:00<09:32, 15.85it/s]\u001b[A\n",
      "  0%|          | 5/9078 [00:00<08:26, 17.91it/s]\u001b[A\n",
      "  0%|          | 8/9078 [00:00<07:52, 19.19it/s]\u001b[A\n",
      "  0%|          | 11/9078 [00:00<07:36, 19.88it/s]\u001b[A\n",
      "  0%|          | 15/9078 [00:00<07:00, 21.53it/s]\u001b[A\n",
      "  0%|          | 17/9078 [00:00<07:32, 20.02it/s]\u001b[A\n",
      "  0%|          | 19/9078 [00:00<08:29, 17.78it/s]\u001b[A\n",
      "  0%|          | 21/9078 [00:01<08:46, 17.20it/s]\u001b[A\n",
      "  0%|          | 23/9078 [00:01<09:31, 15.85it/s]\u001b[A\n",
      "  0%|          | 26/9078 [00:01<08:17, 18.19it/s]\u001b[A\n",
      "  0%|          | 29/9078 [00:01<08:25, 17.90it/s]\u001b[A\n",
      "  0%|          | 31/9078 [00:01<10:15, 14.70it/s]\u001b[A\n",
      "  0%|          | 34/9078 [00:01<09:22, 16.08it/s]\u001b[A\n",
      "  0%|          | 36/9078 [00:01<10:35, 14.24it/s]\u001b[A\n",
      "  0%|          | 40/9078 [00:02<08:43, 17.25it/s]\u001b[A\n",
      "  0%|          | 43/9078 [00:02<08:05, 18.63it/s]\u001b[A\n",
      "  1%|          | 46/9078 [00:02<07:49, 19.25it/s]\u001b[A\n",
      "  1%|          | 49/9078 [00:02<07:27, 20.18it/s]\u001b[A\n",
      "  1%|          | 53/9078 [00:02<06:45, 22.23it/s]\u001b[A\n",
      "  1%|          | 57/9078 [00:02<06:00, 25.01it/s]\u001b[A\n",
      "  1%|          | 62/9078 [00:02<05:12, 28.86it/s]\u001b[A\n",
      "  1%|          | 66/9078 [00:03<06:12, 24.18it/s]\u001b[A\n",
      "  1%|          | 69/9078 [00:03<07:34, 19.83it/s]\u001b[A\n",
      "  1%|          | 72/9078 [00:03<08:19, 18.02it/s]\u001b[A\n",
      "  9%|▉         | 838/9078 [05:09<1:00:25,  2.27it/s] "
     ]
    }
   ],
   "source": [
    "data_generated = pd.DataFrame(columns=data.columns)\n",
    "data_generated['generated_question'] = ''\n",
    "\n",
    "count = 0\n",
    "for paragraph, paragraph_group in tqdm.tqdm(data.groupby('paragraph')):\n",
    "    \n",
    "    for row in paragraph_group.iterrows():\n",
    "        question = row[1]['question'].strip('?').strip(' ')\n",
    "        question_words = [x for x in question.split(' ')]\n",
    "        if int(len(question_words) / 2) <= 1: \n",
    "            continue\n",
    "        \n",
    "        # pick chunk to swap\n",
    "        word_to_swap = np.random.randint(1, int(len(question_words) / 2))\n",
    "        index_to_swap = question.find(' ' + question_words[word_to_swap])\n",
    "\n",
    "        # iterate through question pool\n",
    "        questions_pool = paragraph_group[paragraph_group.question != row[1]['question']].question.values\n",
    "        if len(questions_pool) == 0:\n",
    "            continue\n",
    "            \n",
    "        time_to_generate = np.random.randint(2, 8)\n",
    "        for i in range(time_to_generate):\n",
    "\n",
    "            generated_question = question[:index_to_swap]\n",
    "            time_to_swap = np.random.randint(2, 8)\n",
    "\n",
    "            for i in range(time_to_swap):\n",
    "                pool_question = questions_pool[np.random.randint(0, len(questions_pool))]\n",
    "                pool_question_words = [x for x in pool_question.split(' ')]\n",
    "                if len(pool_question_words) < 3: \n",
    "                    i -= 1\n",
    "                    continue\n",
    "\n",
    "                generated_question = generated_question.strip('?').strip(' ')\n",
    "\n",
    "                # pick chunk to swap\n",
    "                pool_word_to_swap = np.random.randint(1, len(pool_question_words) - 1)\n",
    "                if pool_question_words[pool_word_to_swap] == '?':\n",
    "                    continue \n",
    "                pool_index_to_swap = pool_question.find(' ' + pool_question_words[pool_word_to_swap])\n",
    "\n",
    "                # swap\n",
    "                generated_question += pool_question[pool_index_to_swap:]\n",
    "\n",
    "            if generated_question != row[1]['question'] and len(generated_question) > len(question):\n",
    "                data_generated.loc[count] = [row[1]['paragraph_id'], row[1]['question_id'], paragraph, row[1]['question'], 0, generated_question]\n",
    "                count += 1\n",
    "\n",
    "    if count >= 40000:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_generated = data_generated[data_generated['generated_question'] != '']\n",
    "data_generated[['paragraph_id', 'paragraph', 'generated_question']].drop_duplicates().dropna().to_csv('../data/taskA_generated_questions_p_3.csv', sep=';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**p1 - соединение по слову** - 91.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = ['ADJF+NOUN','PRTS+NOUN', 'NOUN+NOUN', 'NOUN+VERB', 'VERB+ADJF']\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def match(ngram, patterns = patterns):\n",
    "    index = []\n",
    "    for word in ngram:\n",
    "        buf = [(p.normal_form, p.tag.POS) for p in morph.parse(word)]\n",
    "        index.append((word,buf))\n",
    "    pos_tagging = product(*[ind[1] for ind in index])\n",
    "    possible_patterns = map(lambda pos_tag: list(zip(*pos_tag)), pos_tagging)\n",
    "    possible_patterns = map(lambda pattern: [pattern[0], map(lambda grammeme: grammeme, pattern[1])], possible_patterns)\n",
    "    possible_patterns = map(lambda pattern: (pattern[0], '+'.join(pattern[1])), possible_patterns)\n",
    "    for pattern in possible_patterns:\n",
    "        if pattern[1] in patterns:\n",
    "            return pattern\n",
    "    return None        \n",
    "\n",
    "def get_pattern(text, patterns = patterns):\n",
    "    tokens = [x for x in normalize(text)]\n",
    "    pattern_coll2  = {}\n",
    "    coll2 = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    c2_index = -1\n",
    "    for c2 in coll2:\n",
    "        c2_index += 1\n",
    "        try:\n",
    "            p = match(c2, patterns)\n",
    "            if p != None:\n",
    "                collocation = ' '.join(p[0])\n",
    "                if collocation not in pattern_coll2:\n",
    "                    pattern_coll2[collocation] = [p[1], c2_index]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pattern_coll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9078 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 16/9078 [00:00<00:58, 156.05it/s]\u001b[A\n",
      "  0%|          | 30/9078 [00:00<01:00, 150.48it/s]\u001b[A\n",
      "  1%|          | 46/9078 [00:00<01:00, 148.38it/s]\u001b[A\n",
      "  1%|          | 63/9078 [00:00<00:59, 151.99it/s]\u001b[A\n",
      "  1%|          | 78/9078 [00:00<00:59, 151.27it/s]\u001b[A\n",
      "  1%|          | 91/9078 [00:00<01:05, 137.67it/s]\u001b[A\n",
      "  1%|          | 104/9078 [00:00<01:08, 131.19it/s]\u001b[A\n",
      "  1%|▏         | 120/9078 [00:00<01:04, 138.10it/s]\u001b[A\n",
      "  1%|▏         | 134/9078 [00:00<01:10, 126.26it/s]\u001b[A\n",
      "  2%|▏         | 147/9078 [00:01<01:11, 125.04it/s]\u001b[A\n",
      "  2%|▏         | 162/9078 [00:01<01:08, 130.95it/s]\u001b[A\n",
      "  2%|▏         | 176/9078 [00:01<01:14, 120.04it/s]\u001b[A\n",
      "  2%|▏         | 193/9078 [00:01<01:08, 130.55it/s]\u001b[A\n",
      "  2%|▏         | 207/9078 [00:01<01:09, 126.94it/s]\u001b[A\n",
      "  2%|▏         | 220/9078 [00:01<01:12, 122.61it/s]\u001b[A\n",
      "  3%|▎         | 234/9078 [00:01<01:10, 126.30it/s]\u001b[A\n",
      "  3%|▎         | 247/9078 [00:01<01:10, 124.89it/s]\u001b[A\n",
      "  3%|▎         | 264/9078 [00:01<01:05, 134.79it/s]\u001b[A\n",
      "  3%|▎         | 280/9078 [00:02<01:02, 140.46it/s]\u001b[A\n",
      "  3%|▎         | 295/9078 [00:02<01:06, 131.53it/s]\u001b[A\n",
      "  3%|▎         | 309/9078 [00:02<01:20, 109.10it/s]\u001b[A\n",
      "  4%|▎         | 321/9078 [00:02<01:18, 111.76it/s]\u001b[A\n",
      "  4%|▎         | 333/9078 [00:02<01:52, 77.89it/s] \u001b[A\n",
      "  4%|▍         | 343/9078 [00:02<01:48, 80.55it/s]\u001b[A\n",
      "  4%|▍         | 353/9078 [00:02<01:48, 80.14it/s]\u001b[A\n",
      "  4%|▍         | 362/9078 [00:03<01:46, 81.98it/s]\u001b[A\n",
      "  4%|▍         | 373/9078 [00:03<01:46, 81.84it/s]\u001b[A\n",
      "  4%|▍         | 383/9078 [00:03<01:41, 85.44it/s]\u001b[A\n",
      "  4%|▍         | 394/9078 [00:03<01:36, 89.74it/s]\u001b[A\n",
      "  4%|▍         | 405/9078 [00:03<01:33, 92.85it/s]\u001b[A\n",
      "  5%|▍         | 417/9078 [00:03<01:27, 99.15it/s]\u001b[A\n",
      "  5%|▍         | 432/9078 [00:03<01:19, 108.23it/s]\u001b[A\n",
      "  5%|▍         | 444/9078 [00:03<01:20, 107.06it/s]\u001b[A\n",
      "  5%|▌         | 456/9078 [00:03<01:20, 107.08it/s]\u001b[A\n",
      "  5%|▌         | 467/9078 [00:04<01:20, 106.35it/s]\u001b[A\n",
      "  5%|▌         | 479/9078 [00:04<01:18, 109.72it/s]\u001b[A\n",
      "  5%|▌         | 491/9078 [00:04<01:21, 105.77it/s]\u001b[A\n",
      "  6%|▌         | 503/9078 [00:04<01:19, 108.45it/s]\u001b[A\n",
      "  6%|▌         | 518/9078 [00:04<01:12, 117.55it/s]\u001b[A\n",
      "  6%|▌         | 531/9078 [00:04<01:16, 111.07it/s]\u001b[A\n",
      "  6%|▌         | 543/9078 [00:04<01:17, 110.21it/s]\u001b[A\n",
      "  6%|▌         | 555/9078 [00:04<01:17, 109.33it/s]\u001b[A\n",
      "  6%|▋         | 569/9078 [00:04<01:13, 116.05it/s]\u001b[A\n",
      "100%|██████████| 9078/9078 [01:30<00:00, 100.67it/s]\n"
     ]
    }
   ],
   "source": [
    "data_generated = pd.DataFrame(columns=data.columns)\n",
    "data_generated['generated_question'] = ''\n",
    "\n",
    "count = 0\n",
    "for paragraph, paragraph_group in tqdm.tqdm(data.groupby('paragraph')):\n",
    "    \n",
    "    for row in paragraph_group.iterrows():\n",
    "        question = row[1]['question'].strip('?').strip(' ')\n",
    "        question_words = [x for x in question.split(' ') if len(x) > 3]\n",
    "        if len(question_words) < 4:\n",
    "            continue\n",
    "        \n",
    "        # iterate through question pool\n",
    "        questions_pool = paragraph_group[paragraph_group.question != row[1]['question']].question.values\n",
    "        if len(questions_pool) == 0:\n",
    "            continue\n",
    "            \n",
    "        for pool_question in questions_pool:\n",
    "        \n",
    "            # pick chunk to swap\n",
    "            word_to_swap = np.random.randint(1, len(question_words) - 2)\n",
    "            if question_words[word_to_swap] == '?':\n",
    "                continue \n",
    "\n",
    "            index_to_swap = question.find(' ' + question_words[word_to_swap])\n",
    "        \n",
    "            # swap\n",
    "            generated_question = question[:index_to_swap]\n",
    "            pool_index_to_swap = pool_question.find(' ' + question_words[word_to_swap])\n",
    "            \n",
    "            if pool_index_to_swap != -1:\n",
    "                generated_question += pool_question[pool_index_to_swap:]\n",
    "                    \n",
    "                if generated_question != row[1]['question'] and len(generated_question) > len(question):\n",
    "                    data_generated.loc[count] = [row[1]['paragraph_id'], row[1]['question_id'], paragraph, row[1]['question'], row[1]['answer'], generated_question]\n",
    "                    count += 1\n",
    "            \n",
    "    if count >= 20000:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_generated = data_generated[data_generated['generated_question'] != '']\n",
    "data_generated[['paragraph_id', 'paragraph', 'generated_question']].drop_duplicates().dropna().to_csv('../data/taskA_generated_questions_p_1.csv', sep=';', index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
